{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Activation, Conv2D, Conv2DTranspose, LeakyReLU, Flatten, Input, Concatenate\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.data import Dataset\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.gpu.set_per_process_memory_growth(True)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "HEIGHT, WIDTH, CHANNEL = 128, 128, 3\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['grass', 'fire', 'water', 'electric', 'psychic', 'dragon', 'flying']\n",
    "\n",
    "types_idx = {typ: idx for idx, typ in enumerate(types)}\n",
    "df = pd.read_csv('pokemon.csv')[['pokedex_number', 'type1', 'type2']]\n",
    "df = df[df['type1'].isin(types) | df['type2'].isin(types)]\n",
    "n = len(types)\n",
    "def make_cond(r):\n",
    "    z, t1, t2 = np.zeros(n), r[1], r[2]\n",
    "    if t1 in types: z[types_idx[t1]] = 1\n",
    "    if t2 in types: z[types_idx[t2]] = 1\n",
    "    return z\n",
    "df['condition'] = df.apply(make_cond, axis=1)\n",
    "df.set_index('pokedex_number', inplace=True)\n",
    "COND = df.drop(['type1', 'type2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN:\n",
    "    def __init__(self):\n",
    "        self.noise_len = 100\n",
    "        self.noise_dim = (self.noise_len, )\n",
    "        self.in_dim = (128, 128, 3)\n",
    "        self.cond_len = len(types)\n",
    "        self.cond_dim = (self.cond_len, )\n",
    "        self.gen = self.get_gen()\n",
    "        self.dis = self.get_dis()\n",
    "        self.gen_opt = RMSprop(lr=2e-4)\n",
    "        self.dis_opt = RMSprop(lr=2e-4, clipvalue=.01)\n",
    "        self.dis_iters = 5\n",
    "        self.gen_iters = 1\n",
    "        self.ds, self.ds_len = WGAN.process_data()\n",
    "        self.dsi = iter(self.ds)\n",
    "        self.epochs = int(6e4)\n",
    "        self.num_batches = self.ds_len // BATCH_SIZE\n",
    "        self.loss = []\n",
    "\n",
    "    def get_gen(self):\n",
    "        inp = Input(shape=self.noise_dim)\n",
    "        cond = Input(shape=self.cond_dim)\n",
    "        m = Concatenate()([inp, cond])\n",
    "        m = Dense(8192, kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = Reshape((4, 4, 512))(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        m = Activation('relu')(m)\n",
    "        m = Conv2DTranspose(256, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        m = Activation('relu')(m)\n",
    "        m = Conv2DTranspose(128, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        m = Activation('relu')(m)\n",
    "        m = Conv2DTranspose(64, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        m = Activation('relu')(m)\n",
    "        m = Conv2DTranspose(32, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        m = Activation('relu')(m)\n",
    "        out = Conv2DTranspose(3, (5, 5), (2, 2), 'same', activation='sigmoid', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        return Model(inputs=[inp, cond], outputs=out)\n",
    "\n",
    "    def get_dis(self):\n",
    "        inp = Input(shape=self.in_dim)\n",
    "        cond = Input(shape=self.cond_dim)\n",
    "        x = Conv2D(64, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(stddev=.02))(inp)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = Conv2D(128, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(x)\n",
    "        x = BatchNormalization(-1, .9, 1e-5)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = Conv2D(256, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(x)\n",
    "        x = BatchNormalization(-1, .9, 1e-5)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = Conv2D(512, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(x)\n",
    "        x = BatchNormalization(-1, .9, 1e-5)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = Flatten()(x)\n",
    "        m = Concatenate()([x, cond])\n",
    "        m = Dense(512, activation='relu')(m)\n",
    "        out = Dense(1, kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        return Model(inputs=[inp, cond], outputs=out)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            start = time.time()\n",
    "            dis_loss, gen_loss = self.train_step()\n",
    "            self.loss.append((gen_loss, dis_loss))\n",
    "            display.clear_output(wait=True)\n",
    "            noise = tf.random.uniform([64, self.noise_len], -1., 1.)\n",
    "            seed = [noise, self.get_conds()]\n",
    "            self.gen_save(epoch + 1, seed)\n",
    "            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "            print('Epoch: %d, Dis. Loss: %.4f, Gen. Loss: %.4f' % (epoch+1, dis_loss, gen_loss))\n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                self.gen.save('./weights/gen.h5')\n",
    "                self.dis.save('./weights/dis.h5')\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                plt.plot(self.loss)\n",
    "                plt.title('Training Loss for Conditional WGAN')\n",
    "                plt.ylabel('Training Loss')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.legend(['Generator', 'Discriminator'], loc='upper_left')\n",
    "                plt.savefig('train_loss.png', bbox_inches='tight')\n",
    "                plt.clf()\n",
    "        display.clear_output(wait=True)\n",
    "        WGAN.gen_save(self.gen, self.epochs, seed)\n",
    "\n",
    "    # @tf.function\n",
    "    def train_step(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            # train discriminator\n",
    "            noise = tf.random.uniform([BATCH_SIZE, self.noise_len], -1., 1.)\n",
    "            for _ in range(self.dis_iters):\n",
    "                imgs, conds = next(self.dsi)\n",
    "                gen_imgs = self.gen([noise, conds], training=True)\n",
    "                with tf.GradientTape() as dis_t:\n",
    "                    real_out = self.dis([imgs, conds], training=True)\n",
    "                    fake_out = self.dis([gen_imgs, conds], training=True)\n",
    "                    dis_loss = tf.reduce_mean(fake_out) - tf.reduce_mean(real_out)\n",
    "                dis_grad = dis_t.gradient(dis_loss, self.dis.trainable_variables)\n",
    "                self.dis_opt.apply_gradients(zip(dis_grad, self.dis.trainable_variables))\n",
    "            # train generator\n",
    "            for _ in range(self.gen_iters):\n",
    "                conds = self.get_conds()\n",
    "                with tf.GradientTape() as gen_t:\n",
    "                    gen_imgs = self.gen([noise, conds], training=True)\n",
    "                    fake_out = self.dis([gen_imgs, conds], training=True)\n",
    "                    gen_loss = -tf.reduce_mean(fake_out)\n",
    "                gen_grad = gen_t.gradient(gen_loss, self.gen.trainable_variables)\n",
    "                self.gen_opt.apply_gradients(zip(gen_grad, self.gen.trainable_variables))\n",
    "        return dis_loss, gen_loss\n",
    "\n",
    "    @tf.function\n",
    "    def get_conds(self):\n",
    "        nr = 2 if random.random() > 0.4 else 1\n",
    "        idx = tf.random.uniform([BATCH_SIZE, nr], 0, self.cond_len, dtype=tf.int32)\n",
    "        oh = tf.one_hot(idx, self.cond_len)\n",
    "        return tf.clip_by_value(tf.reduce_sum(oh, 1), 0, 1)\n",
    "\n",
    "    def gen_save(self, epoch, test_inp):\n",
    "        pred = self.gen(test_inp, training=False)\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        for i in range(pred.shape[0]):\n",
    "            plt.subplot(8, 8, i+1)\n",
    "            plt.imshow(pred[i])\n",
    "            plt.axis('off')\n",
    "        if epoch % 50 == 0:\n",
    "            plt.savefig('gen_imgs/img_at_ep_{:06d}.png'.format(epoch), bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def process_data():\n",
    "        current_dir = os.getcwd()\n",
    "        pokemon_dir = os.path.join(current_dir, 'data_big')\n",
    "        images, conds = [], []\n",
    "        for each in os.listdir(pokemon_dir):\n",
    "            e = os.path.splitext(each)\n",
    "            if (e[1] == '.jpg') and (int(e[0]) in COND.index):\n",
    "                images.append(os.path.join(pokemon_dir,each))\n",
    "                conds.append(tf.convert_to_tensor(COND.loc[int(e[0])][0], dtype=tf.float32))\n",
    "        image_paths = Dataset.from_tensor_slices((images, conds))\n",
    "        def load_preprocess(path, cond):\n",
    "            content = tf.io.read_file(path)\n",
    "            image = tf.image.decode_jpeg(content, channels=CHANNEL)\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_brightness(image, max_delta=.1)\n",
    "            image = tf.image.random_contrast(image, lower=.9, upper=1.1)\n",
    "            size = [HEIGHT, WIDTH]\n",
    "            image = tf.image.resize(image, size)\n",
    "            image.set_shape([HEIGHT,WIDTH,CHANNEL])\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image /= 255.\n",
    "            return image, cond\n",
    "        all_images = image_paths.map(load_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "        num_images = len(images)\n",
    "        all_images = all_images.shuffle(buffer_size=num_images)\n",
    "        all_images = all_images.repeat()\n",
    "        all_images = all_images.batch(BATCH_SIZE)\n",
    "        all_images = all_images.prefetch(buffer_size=AUTOTUNE)\n",
    "        return all_images, num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan = CGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
