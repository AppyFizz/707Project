{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Activation, Conv2D, Conv2DTranspose, LeakyReLU, Flatten, Input, Concatenate, UpSampling2D\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.gpu.set_per_process_memory_growth(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "HEIGHT, WIDTH, CHANNEL = 128, 128, 3\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeDRAGAN:\n",
    "    def __init__(self):\n",
    "        self.noise_len = 100\n",
    "        self.noise_dim = (self.noise_len, )\n",
    "        self.in_dim = (128, 128, 3)\n",
    "        self.gen = self.get_gen()\n",
    "        self.dis = self.get_dis()\n",
    "        self.gen_opt = Adam(2e-4, .5, .9)\n",
    "        self.dis_opt = Adam(2e-4, .5, .9)\n",
    "        self.dis_iters = 5\n",
    "        self.gen_iters = 1\n",
    "        self.ds, self.ds_len = WGAN.process_data()\n",
    "        self.dsi = iter(self.ds)\n",
    "        self.epochs = int(6e4)\n",
    "        self.num_batches = self.ds_len // BATCH_SIZE\n",
    "        self.weight_dir = 'weights/'\n",
    "        self.weight_pref = 'animedragan'\n",
    "        if not os.path.exists(self.weight_dir):\n",
    "            os.makedirs(self.weight_dir)\n",
    "        self.img_dir = 'animedragan_gen_imgs/'\n",
    "        if not os.path.exists(self.img_dir):\n",
    "            os.makedirs(self.img_dir)\n",
    "        self.loss = []\n",
    "\n",
    "    def gen_conv_block(self, x):\n",
    "        m = Conv2D(64, (3, 3), (1, 1), 'same', kernel_initializer=TruncatedNormal(0, .02))(x)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        m = Activation('relu')(m)\n",
    "        m = Conv2D(64, (3, 3), (1, 1), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        return m + x\n",
    "    \n",
    "    def dis_conv_block(self, x, n):\n",
    "        m = Conv2D(n, (3, 3), (1, 1), 'same', kernel_initializer=TruncatedNormal(0, .02))(x)\n",
    "        m = LeakyReLU(.2)(m)\n",
    "        m = Conv2D(n, (3, 3), (1, 1), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = m + x\n",
    "        return LeakyReLU(.2)(m)\n",
    "    \n",
    "    def upsampling(self, x):\n",
    "        m = Conv2D(256, (3, 3), (1, 1), 'same', kernel_initializer=TruncatedNormal(0, .02))(x)\n",
    "        m = UpSampling2D(2)(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        return Activation('relu')(m)\n",
    "    \n",
    "    def get_gen(self):\n",
    "        inp = Input(shape=self.noise_dim)\n",
    "        m = Dense(16384, kernel_initializer=TruncatedNormal(0, .02))(inp)\n",
    "        m = Reshape((16, 16, 64))(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        m = Activation('relu')(m)\n",
    "        x = m\n",
    "        for _ in range(16): m = self.gen_conv_block(m)\n",
    "        m = BatchNormalization(-1, .9, 1e-5)(m)\n",
    "        m = Activation('relu')(m)\n",
    "        m = m + x\n",
    "        for _ in range(3): m = self.upsampling(m)\n",
    "        out = Conv2D(3, (9, 9), (1, 1), 'same', activation='sigmoid', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        return Model(inputs=inp, outputs=out)\n",
    "\n",
    "    def get_dis(self):\n",
    "        inp = Input(shape=self.in_dim)\n",
    "        m = Conv2D(64, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(stddev=.02))(inp)\n",
    "        m = LeakyReLU(.2)(m)\n",
    "        m = self.dis_conv_block(m, 64)\n",
    "        m = Conv2D(128, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = LeakyReLU(.2)(m)\n",
    "        m = self.dis_conv_block(m, 128)\n",
    "        m = Conv2D(256, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = LeakyReLU(.2)(m)\n",
    "        m = self.dis_conv_block(m, 256)\n",
    "        m = Conv2D(512, (5, 5), (2, 2), 'same', kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        m = LeakyReLU(.2)(m)\n",
    "        m = Flatten()(m)\n",
    "        out = Dense(1, kernel_initializer=TruncatedNormal(0, .02))(m)\n",
    "        return Model(inputs=inp, outputs=out)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            start = time.time()\n",
    "            dis_loss, gen_loss = self.train_step()\n",
    "            self.loss.append((gen_loss, dis_loss))\n",
    "            display.clear_output(wait=True)\n",
    "            seed = tf.random.normal([64, self.noise_len])\n",
    "            self.gen_save(epoch + 1, seed)\n",
    "            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "            print('Epoch: %d, Dis. Loss: %.4f, Gen. Loss: %.4f' % (epoch+1, dis_loss, gen_loss))\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.gen.save_weights(self.weight_dir + self.weight_pref + '_gen.h5')\n",
    "                self.dis.save_weights(self.weight_dir + self.weight_pref + '_dis.h5')\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                plt.plot(self.loss)\n",
    "                plt.title('Training Loss for Anime DRAGAN')\n",
    "                plt.ylabel('Training Loss')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.legend(['Generator', 'Discriminator'], loc='upper_left')\n",
    "                plt.savefig('train_loss.png', bbox_inches='tight')\n",
    "                plt.clf()\n",
    "        display.clear_output(wait=True)\n",
    "        self.gen_save(self.epochs, seed)\n",
    "    \n",
    "    def train_step(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            noise = tf.random.normal([BATCH_SIZE, self.noise_len])\n",
    "            imgs = next(self.dsi)\n",
    "            std = tf.sqrt(tf.nn.moments(imgs, [0])[1])\n",
    "            U = tf.random.uniform(self.in_dim, 0, .5)\n",
    "            imgs_p = imgs + std * U\n",
    "            a = tf.random.uniform([BATCH_SIZE, 1, 1, 1], 0, 1)\n",
    "            imgs_h = a * imgs + (1 - a) * imgs_p\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "                real_out = self.dis(imgs, training=True)\n",
    "                gen_imgs = self.gen(noise, training=True)\n",
    "                fake_out = self.dis(gen_imgs, training=True)\n",
    "                with tf.GradientTape() as inner_tape:\n",
    "                    inner_tape.watch(imgs_h)\n",
    "                    realh_out = self.dis(imgs_h, training=True)\n",
    "                dish_grad = inner_tape.gradient(realh_out, [imgs_h])[0]\n",
    "                dis_loss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                labels=tf.ones_like(real_out),\n",
    "                                logits=real_out\n",
    "                            ))\n",
    "                dis_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                labels=tf.zeros_like(fake_out),\n",
    "                                logits=fake_out\n",
    "                            ))              \n",
    "                slope = tf.sqrt(tf.reduce_sum(tf.square(dish_grad), axis=[1, 2, 3]))\n",
    "                dis_loss += 10 * tf.reduce_mean((slope-1.)**2)\n",
    "                gen_loss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                labels=tf.ones_like(fake_out),\n",
    "                                logits=fake_out\n",
    "                            ))\n",
    "            gen_grad = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
    "            dis_grad = dis_tape.gradient(dis_loss, self.dis.trainable_variables)\n",
    "            self.gen_opt.apply_gradients(zip(gen_grad, self.gen.trainable_variables))\n",
    "            self.dis_opt.apply_gradients(zip(dis_grad, self.dis.trainable_variables))\n",
    "        return dis_loss, gen_loss\n",
    "\n",
    "    def gen_save(self, epoch, test_inp):\n",
    "        pred = self.gen(test_inp, training=False)\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        for i in range(pred.shape[0]):\n",
    "            plt.subplot(8, 8, i+1)\n",
    "            plt.imshow(pred[i])\n",
    "            plt.axis('off')\n",
    "        if epoch % 50 == 0:\n",
    "            plt.savefig(self.img_dir + 'img_at_ep_{:06d}.png'.format(epoch), bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def process_data():\n",
    "        current_dir = os.getcwd()\n",
    "        pokemon_dir = os.path.join(current_dir, 'data_huge')\n",
    "        images = []\n",
    "        for each in os.listdir(pokemon_dir):\n",
    "            images.append(os.path.join(pokemon_dir,each))\n",
    "        image_paths = Dataset.from_tensor_slices(images)\n",
    "        def load_preprocess(path):\n",
    "            content = tf.io.read_file(path)\n",
    "            image = tf.image.decode_jpeg(content, channels=CHANNEL)\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_brightness(image, max_delta=.1)\n",
    "            image = tf.image.random_contrast(image, lower=.9, upper=1.1)\n",
    "            size = [HEIGHT, WIDTH]\n",
    "            image = tf.image.resize(image, size)\n",
    "            image.set_shape([HEIGHT,WIDTH,CHANNEL])\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image /= 255.\n",
    "            return image\n",
    "        all_images = image_paths.map(load_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "        num_images = len(images)\n",
    "        all_images = all_images.shuffle(buffer_size=num_images)\n",
    "        all_images = all_images.repeat()\n",
    "        all_images = all_images.batch(BATCH_SIZE)\n",
    "        all_images = all_images.prefetch(buffer_size=AUTOTUNE)\n",
    "        return all_images, num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agan = AnimeDRAGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agan.gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agan.dis.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
